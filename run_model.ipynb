{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.classifier import TransformerEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"hidden_size\": 48,  # size of the hidden layers and embeddings\n",
    "    \"hidden_ff\": 96,  # size of the position-wise feed-forward layer\n",
    "    \"n_encoders\": 4,  # number of encoder blocks\n",
    "    \"n_heads\": 2,  # number of attention heads in the multiheadattention module\n",
    "    \"n_local\": 2,  # number of local attention heads\n",
    "    \"local_window_size\": 4,  # size of the window for local attention\n",
    "    'batch_size': 4,\n",
    "    \"max_length\": 30,  # maximum length of the input sequence\n",
    "    \"vocab_size\": 100,  # size of the vocabulary\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 30,\n",
    "    \"attention_type\": \"performer\",\n",
    "    \"norm_type\": \"rezero\",\n",
    "    \"num_random_features\": 32,  # number of random features for the Attention module (Performer uses this)\n",
    "    \"emb_dropout\": 0.1,  # dropout for the embedding block\n",
    "    \"fw_dropout\": 0.1,  # dropout for the position-wise feed-forward layer\n",
    "    \"att_dropout\": 0.1,  # dropout for the multiheadattention module\n",
    "    \"dc_dropout\": 0.1,  # dropout for the decoder block\n",
    "    \"hidden_act\": \"swish\",  # activation function for the hidden layers (attention layers use ReLU)\n",
    "    \"epsilon\": 1e-8,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "}\n",
    "model = TransformerEncoder(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "Check if returns values and if the output looks OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    'tokens': torch.randint(0, model.hparams.vocab_size, (model.hparams.batch_size, model.hparams.max_length)),\n",
    "    'abspos': torch.arange(0, model.hparams.max_length).unsqueeze(0).repeat(model.hparams.batch_size, 1),\n",
    "    'age': torch.randint(0, 100, (model.hparams.batch_size, 1)).repeat(1, model.hparams.max_length),\n",
    "    'padding_mask': torch.zeros(model.hparams.batch_size, model.hparams.max_length),\n",
    "    'targets': torch.randint(0, 2, (model.hparams.batch_size, 1)).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[70, 33, 44, 72,  9, 40, 55, 65,  8, 67, 70, 92, 48, 22, 12, 22, 59, 52,\n",
       "          28, 31, 74, 71, 51, 92,  3, 98, 18, 76, 26, 92],\n",
       "         [28, 24, 22, 22, 81, 28, 71, 65,  2, 46, 28,  4, 62, 71, 12, 36, 36, 93,\n",
       "          50, 74, 47,  3, 73, 89, 56, 79,  4, 87, 37, 89],\n",
       "         [72, 69, 64, 11, 85, 24,  6, 40, 87,  6, 69, 62,  3, 31, 82,  3, 87, 58,\n",
       "          39, 38, 76, 39, 12, 54,  1, 48, 95,  7, 51, 68],\n",
       "         [88, 45, 71, 68, 46, 65, 93, 45, 54, 92, 98, 50, 17, 21, 37, 36, 90, 27,\n",
       "          88, 48, 35, 21, 80, 25, 98, 99, 58, 48, 67, 28]]),\n",
       " 'abspos': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]),\n",
       " 'age': tensor([[ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "           3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "         [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "          60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
       "         [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "          18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18],\n",
       "         [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "           8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8]]),\n",
       " 'padding_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]]),\n",
       " 'targets': tensor([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-1.0735],\n",
       "         [-0.7359],\n",
       "         [-0.9112],\n",
       "         [-0.5476]], grad_fn=<AddmmBackward0>),\n",
       " 'preds': tensor([[0.2547],\n",
       "         [0.3239],\n",
       "         [0.2868],\n",
       "         [0.3664]], grad_fn=<SigmoidBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7365, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Metrics\n",
      "\tLoss: 0.737\n",
      "\tAccuracy: 0.500\n",
      "\tMCC: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.on_train_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline would be something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.synthetic import SyntheticDataModule\n",
    "\n",
    "dataloader = SyntheticDataModule(num_samples=1000, max_length=hparams['max_length'],\n",
    "                                  batch_size=hparams['batch_size'], vocab_size=hparams['vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/carlomarx/.local/share/virtualenvs/project2vec-D-jEdA35-python/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_mcc', save_top_k=2, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_mcc', patience=5, mode='max')\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"transformer\")\n",
    "\n",
    "trainer = Trainer(max_epochs=30,\n",
    "                accelerator=\"cpu\",   ### change to \"cuda\" or \"gpu\" or 'msp'\n",
    "                limit_train_batches=0.5,\n",
    "                logger=logger,\n",
    "                accumulate_grad_batches=4,\n",
    "                num_sanity_val_steps=8,\n",
    "                callbacks = [model_checkpoint, early_stopping],\n",
    "                check_val_every_n_epoch=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs/transformer\n",
      "\n",
      "   | Name        | Type                   | Params | Mode\n",
      "---------------------------------------------------------------\n",
      "0  | transformer | Transformer            | 79.5 K | eval\n",
      "1  | decoder     | CLS_Decoder            | 2.4 K  | eval\n",
      "2  | loss        | BCEWithLogitsLoss      | 0      | eval\n",
      "3  | train_loss  | MeanMetric             | 0      | eval\n",
      "4  | val_loss    | MeanMetric             | 0      | eval\n",
      "5  | test_loss   | MeanMetric             | 0      | eval\n",
      "6  | train_acc   | BinaryAccuracy         | 0      | eval\n",
      "7  | val_acc     | BinaryAccuracy         | 0      | eval\n",
      "8  | test_acc    | BinaryAccuracy         | 0      | eval\n",
      "9  | train_mcc   | BinaryMatthewsCorrCoef | 0      | eval\n",
      "10 | val_mcc     | BinaryMatthewsCorrCoef | 0      | eval\n",
      "11 | test_mcc    | BinaryMatthewsCorrCoef | 0      | eval\n",
      "---------------------------------------------------------------\n",
      "81.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "81.9 K    Total params\n",
      "0.328     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 8/8 [00:00<00:00, 105.54it/s]\n",
      "Val Metrics\n",
      "\tLoss: 0.757\n",
      "\tAccuracy: 0.514\n",
      "\tMCC: 0.003\n",
      "\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/.local/share/virtualenvs/project2vec-D-jEdA35-python/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/carlomarx/.local/share/virtualenvs/project2vec-D-jEdA35-python/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [00:01<00:00, 98.67it/s, v_num=0]\n",
      "Val Metrics\n",
      "\tLoss: 0.751\n",
      "\tAccuracy: 0.504\n",
      "\tMCC: 0.000\n",
      "\n",
      "Epoch 0: 100%|██████████| 125/125 [00:02<00:00, 45.61it/s, v_num=0]\n",
      "Train Metrics\n",
      "\tLoss: 0.770\n",
      "\tAccuracy: 0.493\n",
      "\tMCC: -0.001\n",
      "\n",
      "Epoch 1: 100%|██████████| 125/125 [00:01<00:00, 94.90it/s, v_num=0]\n",
      "Val Metrics\n",
      "\tLoss: 0.743\n",
      "\tAccuracy: 0.502\n",
      "\tMCC: 0.000\n",
      "\n",
      "Epoch 1: 100%|██████████| 125/125 [00:02<00:00, 44.23it/s, v_num=0]\n",
      "Train Metrics\n",
      "\tLoss: 0.765\n",
      "\tAccuracy: 0.489\n",
      "\tMCC: -0.001\n",
      "\n",
      "Epoch 2: 100%|██████████| 125/125 [00:01<00:00, 96.60it/s, v_num=0]\n",
      "Val Metrics\n",
      "\tLoss: 0.737\n",
      "\tAccuracy: 0.496\n",
      "\tMCC: -0.000\n",
      "\n",
      "Epoch 2: 100%|██████████| 125/125 [00:02<00:00, 44.13it/s, v_num=0]\n",
      "Train Metrics\n",
      "\tLoss: 0.753\n",
      "\tAccuracy: 0.494\n",
      "\tMCC: -0.001\n",
      "\n",
      "Epoch 3:  86%|████████▌ | 107/125 [00:01<00:00, 95.45it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/.local/share/virtualenvs/project2vec-D-jEdA35-python/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2vec-D-jEdA35-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
