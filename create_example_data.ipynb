{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'person_id': [1,1,1,2,2,3],\n",
    "    'abspos': [100.5, 200.123, 300.3, 400, 500, 600], # abspos in hours since Jan1, 2020 (see utils.py:calculate_abspos.py)\n",
    "    'age': [10, 20, 30, 40, 50, 60], # age in years\n",
    "    'event': [[5, 6], [7], [5, 7, 8, 10], [1,2,3], [1,2,3,4], [1]], # Tokenized tokens, where each list (uneven lengths) is an event, each element in list is token\n",
    "    # Segment is not saved here and would require to be created in collate_fn (if in-memory, we can create it here)\n",
    "})\n",
    "vocabulary = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10}\n",
    "targets = pd.DataFrame({\n",
    "    'person_id': [1,2,3],\n",
    "    'target': [1, 0, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a large dataexample following the structure from above\n",
    "n_persons = 1000\n",
    "data = []\n",
    "for i in range(n_persons):\n",
    "    for j in range(np.random.randint(1, 10)):\n",
    "        data.append({\n",
    "            'person_id': i,\n",
    "            'abspos': 100.5 + j,\n",
    "            'age': 10 + j,\n",
    "            'event': list(np.random.randint(1, 11, np.random.randint(1, 10)))\n",
    "        })\n",
    "df = pd.DataFrame(data)\n",
    "vocabulary = {'[PAD]': 0,'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10}\n",
    "\n",
    "# for each person, get the sum of the event tokens over all event lists\n",
    "person_sum = df.groupby('person_id').apply(lambda x: x['event'].apply(lambda x: sum(x)).sum())\n",
    "target = (person_sum <= person_sum.quantile(0.5)).astype(int)\n",
    "\n",
    "person_id = person_sum.index\n",
    "\n",
    "targets = pd.DataFrame({\n",
    "    'person_id': list(person_id),\n",
    "    'target': target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('fake_data/sequence_data.parquet', index=False)\n",
    "targets.to_csv('fake_data/targets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fake_data/vocab.json', 'w') as f:\n",
    "    json.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
